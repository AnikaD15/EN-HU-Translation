{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25024/1085563809.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mattenvis\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAttentionVis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from attenvis import AttentionVis\n",
    "av = AttentionVis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.path.dirname(__file__)\n",
    "\n",
    "PAD_TOKEN = \"*PAD*\"\n",
    "STOP_TOKEN = \"*STOP*\"\n",
    "START_TOKEN = \"*START*\"\n",
    "UNK_TOKEN = \"*UNK*\"\n",
    "HUNGARIAN_WINDOW_SIZE = 14\n",
    "ENGLISH_WINDOW_SIZE = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_corpus(hungarian, english):\n",
    "\tHUNGARIAN_padded_sentences = []\n",
    "\tHUNGARIAN_sentence_lengths = []\n",
    "\tfor line in hungarian:\n",
    "\t\tpadded_hungarian = line[:HUNGARIAN_WINDOW_SIZE-1]\n",
    "\n",
    "\t\tpadded_hungarian += [STOP_TOKEN] + [PAD_TOKEN] *  (HUNGARIAN_WINDOW_SIZE - len(padded_hungarian)-1)\n",
    "\t\tHUNGARIAN_padded_sentences.append(padded_hungarian)\n",
    "\n",
    "\tENGLISH_padded_sentences = []\n",
    "\tENGLISH_sentence_lengths = []\n",
    "\tfor line in english:\n",
    "\t\tpadded_ENGLISH = line[:ENGLISH_WINDOW_SIZE-1]\n",
    "\t\tpadded_ENGLISH = [START_TOKEN] + padded_ENGLISH + [STOP_TOKEN] + [PAD_TOKEN] * (ENGLISH_WINDOW_SIZE - len(padded_ENGLISH)-1)\n",
    "\t\tENGLISH_padded_sentences.append(padded_ENGLISH)\n",
    "\n",
    "\treturn HUNGARIAN_padded_sentences, ENGLISH_padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_vocab(sentences):\n",
    "\t\"\"\"\n",
    "  Builds vocab from list of sentences\n",
    "\n",
    "\t:param sentences:  list of sentences, each a list of words\n",
    "\t:return: tuple of (dictionary: word --> unique index, pad_token_idx)\n",
    "  \"\"\"\n",
    "\ttokens = []\n",
    "\tfor s in sentences: tokens.extend(s)\n",
    "\tall_words = sorted(list(set([STOP_TOKEN,PAD_TOKEN,UNK_TOKEN] + tokens)))\n",
    "\n",
    "\tvocab =  {word:i for i,word in enumerate(all_words)}\n",
    "\n",
    "\treturn vocab,vocab[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_id(vocab, sentences):\n",
    "\t\"\"\"\n",
    "  Convert sentences to indexed.\n",
    "\n",
    "\t:param vocab:  dictionary. Key: word --> Value: unique index\n",
    "\t:param sentences:  list of lists of words, each representing padded sentence\n",
    "\t:return: numpy array of integers, with each row representing the word indeces in the corresponding sentences\n",
    "  \"\"\"\n",
    "\ttoBeStacked =[]\n",
    "\tfor sentence in sentences:\n",
    "\t\ttokenized=[]\n",
    "\t\tfor word in sentence:\n",
    "\t\t\tif word in vocab:\n",
    "\t\t\t\ttokenized.append(vocab[word])\n",
    "\t\t\telse:\n",
    "\t\t\t\ttokenized.append(vocab[UNK_TOKEN])\n",
    "\t\ttoBeStacked.append(tokenized)\n",
    "\t# return np.stack([[vocab[word] if word in vocab else vocab[UNK_TOKEN] for word in sentence] for sentence in sentences])\n",
    "\treturn np.stack(toBeStacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(referenceFile, file_lang1, file_lang2):\n",
    "\t\"\"\"\n",
    "  Load text data from file\n",
    "\n",
    "\t:param file_name:  string, name of data file\n",
    "\t:return: list of sentences, each a list of words split on whitespace\n",
    "  \"\"\"\n",
    "\tlang1_sentences = open(file_lang1).readlines()\n",
    "\tlang2_sentences = open(file_lang2).readlines()\n",
    "\tlang1_data, lang2_data = [],[]\n",
    "\twith open(referenceFile, 'rt', encoding='latin') as data_file:\n",
    "\t\tfor i in range(6001):\n",
    "\t\t\tline = data_file.readline()\n",
    "\t\t\tindex_lang1 = int(line.split('\\t')[-2].split()[0])-1\n",
    "\t\t\tindex_lang2 = int(line.split('\\t')[-1].split()[0])-1\n",
    "\t\t\tlang1_data.append(lang1_sentences[index_lang1].split())\n",
    "\t\t\tlang2_data.append(lang2_sentences[index_lang2].split())\n",
    "\n",
    "\treturn lang1_data, lang2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@av.get_data_func\n",
    "def get_data(referenceFile, lang1File, lang2File):\n",
    "\ten_train, hu_train = read_data(referenceFile, lang1File, lang2File)\n",
    "\ten_test = en_train[:len(en_train)//2]\n",
    "\thu_test = hu_train[:len(hu_train)//2]\n",
    "\ten_train = en_train[len(en_train)//2:]\n",
    "\thu_train = hu_train[len(hu_train)//2:]\n",
    "\n",
    "\t# pad training/testing data\n",
    "\tpadded_hu_train, padded_en_train = pad_corpus(hu_train, en_train)\n",
    "\tpadded_hu_test, padded_en_test = pad_corpus(hu_test, en_test)\n",
    "\n",
    "\t# build vocab for hungarian\n",
    "\thu_vocab, hu_padding_index = build_vocab(padded_hu_train)\n",
    "\ten_vocab, en_padding_index = build_vocab(padded_en_train)\n",
    "\n",
    "\tprint('finished building vocab')\n",
    "\t# convert training and testing english sentences to list of IDS\n",
    "\ttrain_english = convert_to_id(en_vocab, padded_en_train)\n",
    "\ttest_english = convert_to_id(en_vocab, padded_en_test)\n",
    "\tprint('finished covert to id')\n",
    "\t# convert training and testing hungarian sentences to list of IDS\n",
    "\ttrain_hungarian = convert_to_id(hu_vocab, padded_hu_train)\n",
    "\ttest_hungarian = convert_to_id(hu_vocab, padded_hu_test)\n",
    "\tprint('******* finished get_data')\n",
    "\n",
    "\treturn train_english, test_english, train_hungarian, test_hungarian, en_vocab, hu_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ =='__main__':\n",
    "\tget_data(\n",
    "\t\tos.path.join(dirname, '../data/references.txt'),\n",
    "\t\tos.path.join(dirname, '../data/en-data.txt'),\n",
    "\t\tos.path.join(dirname, '../data/hu-data.txt'))\n",
    "\tprint(\"Preprocessing complete.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd4e8ef4ee21586f696cb2a564891d3bcf09e008ea5d855758ec338bdca8999c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
